{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3237c6b4",
   "metadata": {},
   "source": [
    "# I. Course Overview\n",
    "## 1. Intro to A/B Testing\n",
    "A/B testing is a general methodology used online when people want to test out a **new product or a feature**. \n",
    "\n",
    "A/B tests allow people to determine **scientifically** how to optimize a website or a mobile app by trying out possible changes and seeing what performs better with users. \n",
    "\n",
    "Using A/B tests means that people can get **data** to make decisions rather than relying on intuition or hispsters. \n",
    "\n",
    "The **goal** in A/B testing is to design an experiment, that's going to be **robust** and give people **repeatable** results, so that people can actually make a good decision about whether or not to actually launch that product or feature. \n",
    "\n",
    "#### Full process of an A/B testing:\n",
    "- Choose a metric\n",
    "- Review statistics\n",
    "- Design experiments\n",
    "- Analyze results\n",
    "\n",
    "## 2. Examples of A/B Testing in Industries\n",
    "- Google tested 41 different shades of blue.\n",
    "- Amazon initially decided to launch their first personalized product recommendations based on an A/B test showing a huge revenue increase by adding that feature. \n",
    "- LinkedIn tested whether to use the top slot on a user's stream for top news articles or an encouragement to add more contacts. \n",
    "- Amazon determined that every 100ms increase in page load time decreased sales by 1%. \n",
    "- Googleâ€™s latency results showed a similar impact for a 100ms delay.\n",
    "\n",
    "#### Other Examples:\n",
    "\n",
    "- Kayak tested whether notifying users that their payment was encrypted would make users more or less likely to complete the payment.\n",
    "- Khan Academy tests changes like letting students know how many other students are working on the exercise with them, or making it easier for students to fast-forward past skills they already have. \n",
    "\n",
    "## 3. What you can and can't do with A/B Tests\n",
    "#### When can you use A/B Testing\n",
    "- Add premium service\n",
    "- Move recommendation site: new ranking algorithm\n",
    "- Change backend-page load time, results users see, etc.\n",
    "- Test layout of initial page\n",
    "\n",
    "#### Not suitable: \n",
    "- online shopping company: is my site complete? \n",
    "- Website selling cars: will a change increase repeat customers or referrals?\n",
    "- update brand, including main logo\n",
    "\n",
    "## 4. Other Techniques Apart from A/B Testing\n",
    "- user experience research (eg., check user logs)\n",
    "- focus groups\n",
    "- surveys\n",
    "- human evaluation\n",
    "\n",
    "## 5. A/B Testing Business Case - Audacity \n",
    "\n",
    "##### Background:\n",
    "Audacity is a website that creates online finance courses.\n",
    "\n",
    "Its user flow (also known as **customer funnel**) looks like this: \n",
    "- Homepage visits\n",
    "- Exploring the site\n",
    "- Create account\n",
    "- Complete\n",
    "\n",
    "##### Change we want to make:\n",
    "Change the 'Start Now' button from orange to pink\n",
    "\n",
    "##### Experiment:\n",
    "**Initial Hypothesis**: \n",
    "\n",
    "Changing the \"Start Now\" button from orange to pink will increase how many students explore Audacity courses\n",
    "\n",
    "##### Choose a metric:\n",
    "Refining the hypothesis to see which metric to use\n",
    "- total number of courses completed (not practical)\n",
    "- number of clicks on the \"Start Now\" button (not considering the number of page views)\n",
    "- fraction of clicks: number of clicks/ number of page views; this number is also called **click-through-rate**, or CTR\n",
    "- **click-through-probability**: number of unique visitors who click/ number of unique visitors to page\n",
    "\n",
    "We'll use **click-through-probability** as our metric\n",
    "\n",
    "##### Updated hypothesis:\n",
    "Changing the \"Start Now\" button from orange to pink will increase the **click-through-probability** of the website\n",
    "\n",
    "##### Why click-through-probability over click-through-rate?\n",
    "Generally speaking, we use a rate when we want to measure the *usability* of the site, and a probability when we want to measure the *total impact*. \n",
    "\n",
    "Here in the example, we are interested in whether users are *progressing* to the second level of the funnel, so we picked a probability. \n",
    "\n",
    "##### How to compute the rate/ probabaility?\n",
    "**To compute the rate**: \n",
    "\n",
    "- First, work with the engineers to modify the website. Engineers need to change the website so that on every page view, the event can be captured. And then whenever a user clicks, we can also capture the click event. \n",
    "- Second, after we captured the data, we sum the page views and the clicks, and then we divide to get the rate.\n",
    "\n",
    "**To compute the probability**:\n",
    "\n",
    "count at most 1 child click per page view\n",
    "\n",
    "##### Repeating the Experiement\n",
    "Repeated measurement of click-through-probability\n",
    "\n",
    "**For example**,\n",
    "visitors = 1000, unique clicks = 100\n",
    "\n",
    "--> click-through-probability = 10%\n",
    "\n",
    "Repeat the experiment --> which results would *surprise* us if we repeated the experiment? \n",
    "A. 100\n",
    "\n",
    "B. 103\n",
    "\n",
    "C. 98\n",
    "\n",
    "D. 150\n",
    "\n",
    "E. 900\n",
    "\n",
    "**The answer is D & E.**\n",
    "\n",
    "So what can we use when we decide if the number surprises us?\n",
    "\n",
    "## 6. Review the statistics\n",
    "- binomial distribution\n",
    "- confidence intervals\n",
    "- hypothesis testing\n",
    "\n",
    "### How do we know how variable the estimate is likely to be?\n",
    "**binomial**: two exclusive outcomes\n",
    "\n",
    "#### binomial Distribution\n",
    "eg. biased coin: p = 3/4\n",
    "\n",
    "success = heads, failure = tails\n",
    "\n",
    "mean = p, std dev = square root of (p * (1 - p)/ N)\n",
    "\n",
    "Suppose N = 20, X = 16, \n",
    "\n",
    "then P_hat = 16/20 = 4/5\n",
    "\n",
    "##### When can we use the binomial distribution?\n",
    "1. 2 types of outcomes (success, failure)\n",
    "2. Independent events\n",
    "3. Identical distribution (p is the same for all)\n",
    "\n",
    "##### Examples:\n",
    "- Roll a dice 50 times. Outcomes: 6 or other\n",
    "- Student completion of course after 2 months. Outcomes: Complete or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368fd82",
   "metadata": {},
   "source": [
    "#### Confidence Intervals\n",
    "\n",
    "##### Calculating a confidence interval\n",
    "\n",
    "p_hat = X/N (X: # of users who clicked; N: # of total users)\n",
    "\n",
    "X = 100, N = 1000\n",
    "\n",
    "To use normal distribution: \n",
    "\n",
    "check **N * p_hat > 5** & **N * (1 - p_hat) > 5**\n",
    "\n",
    "m = margin of error = z * std error = z * square root of (p_hat * (1 - p_hat)/ N)\n",
    "\n",
    "z-score = 1.96 for 95% confidence interval\n",
    "\n",
    "--> m = 0.019\n",
    "\n",
    "--> confidence interval = (0.081, 0.119) \n",
    "\n",
    "--> In other words, if we run the experiment on another 1000 users, seeing 80 to 120 clicks would be reasonable\n",
    "\n",
    "##### eg., \n",
    "\n",
    "N = 2000, X = 300, confidence level = 99%, use a two-tailed test. So the CI would be (, ).\n",
    "\n",
    "Answer:\n",
    "\n",
    "z-score = 2.58, mean = 0.15, and thus m = 2.58 * square root (0.15 * 0.85 / 2000) = 0.021\n",
    "\n",
    "So the left of CI = 0.15 - 0.021  = 0.129, and the right of CI = 0.15 + 0.021 = 0.171\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9311d72",
   "metadata": {},
   "source": [
    "##### Establishing Satistical Significance\n",
    "**Hypothesis Testing**\n",
    "* Null and Alternative Hypotheses\n",
    "* P(results due to change)\n",
    "    - Null hypothesis, or H_0: p_cont = p_exp, aka p_exp - p_cont = 0\n",
    "    - Alternative hypothesis, or H_A: p_exp - p_cont is not equal to 0\n",
    "* Measure p_cont_hat and p_exp_hat\n",
    "* Calculate P(p_cont_hat - p_exp_hat|H_0)\n",
    "* Reject null if P < 0.05 (alpha)\n",
    "\n",
    "**Two-tailed vs. one-tailed tests**\n",
    "\n",
    "The null hypothesis and alternative hypothesis proposed here correspond to a two-tailed test, which allows us to distinguish between three cases:\n",
    "\n",
    "- A statistically significant positive result\n",
    "- A statistically significant negative result\n",
    "- No statistically significant difference.\n",
    "\n",
    "Sometimes when people run A/B tests, they will use a one-tailed test, which only allows us to distinguish between two cases:\n",
    "\n",
    "- A statistically significant positive result\n",
    "- No statistically significant result\n",
    "\n",
    "Which one we should use depends on what action we will take based on the results. \n",
    "\n",
    "* If we're going to launch the experiment for a statistically significant positive change, and otherwise not, then we don't need to distinguish between a negative result and no result, so a one-tailed test is good enough. \n",
    "* If we want to learn the direction of the difference, then a two-tailed test is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db52d6",
   "metadata": {},
   "source": [
    "### Comparing two samples\n",
    "Calculate\n",
    "- pooled standard error\n",
    "- X_cont, X_exp\n",
    "- N_cont, N_exp\n",
    "- P_pool_hat = (X_cont + X_exp)/(N_cont + N_exp)\n",
    "- SE_pool = square root of (P_pool_hat * (1 - P_pool_hat)/((1/N_cont) + (1/N_exp)))\n",
    "- d_hat = p_exp_hat - p_cont_hat\n",
    "Give H_0: d = 0, d_hat is N(0, SE_pool)\n",
    "\n",
    "--> If d_hat > 1.96 * SE_pool or d_hat < -1.96 * SE_pool, reject null\n",
    "\n",
    "### Practical or Substantive Significance \n",
    "They mean the same thing. \n",
    "\n",
    "Basically, on top of **statistical significance**, does the change matter to the **business**? \n",
    "\n",
    "Statistical significance is about **repeatability**. \n",
    "\n",
    "So we still need to pick a **practical significance boundary**. \n",
    "\n",
    "For example, for a business, a 2% change in the click-through-probabiliry would be practically significant. \n",
    "\n",
    "#### Design the experiment \n",
    "**statistical power vs size trade off**\n",
    "\n",
    "How many page views\n",
    "\n",
    "alpha = P(reject null|null true)\n",
    "beta = P(fail to reject|null fake)\n",
    "\n",
    "when collecting a small sample --> alpha is low (aka, we are unlikely to launch a bad experiment), but beta is hight (aka we are likely to fail to launch an experiment that actually did have a difference we care about)\n",
    "\n",
    "1 - beta = sensitivity\n",
    "\n",
    "We want our experiment to have a high level of sensitivity at the practical significance boundary, and people usually choose 80%. \n",
    "\n",
    "when collecting a big sample --> alpha the same, but beta is low, so the power increases (aka, more likely to launch the experiment that actually has the difference)\n",
    "\n",
    "#### Calculating number of pages views needed\n",
    "- built-in library\n",
    "- look up answer in a table\n",
    "use online calculater (https://www.evanmiller.org/ab-testing/sample-size.html)\n",
    "##### eg,\n",
    "baseline conversion rate: the estimated click-through-probability before making the change = 0.1 (X = 100, N = 1000)\n",
    "\n",
    "minimum detectable effect = practical significance level = 5%\n",
    "\n",
    "The Minimum Detectable Effect is the smallest effect that will be detected (1-Î²)% of the time.\n",
    "\n",
    "Conversion rates in the gray area (baseline conversion rate +- minimum detectable effect) will not be distinguishable from the baseline.\n",
    "\n",
    "Statistical power = 1 - beta, it means the percent of the time the minimum effect size will be detected, assuming it exists\n",
    "\n",
    "Significance level alpha: Percent of the time a difference will be detected, assuming one does NOT exist\n",
    "\n",
    "##### Note on power\n",
    "Statistics textbooks frequently define power to mean the same thing as sensitivity, that is, 1 - beta. \n",
    "\n",
    "However, conversationally power often means the probability that your test draws the correct conclusions, and this probability depends on both alpha and beta. \n",
    "\n",
    "In this course, we'll use the second definition, and we'll use sensitivity to refer to 1 - beta.\n",
    "\n",
    "##### Should we increase or decrease number of page views if we want to keep everything else the same after the change?\n",
    "Change\n",
    "1. Higher click-through-probability in control (but still less than 0.5) \n",
    "--> increase # of page views\n",
    "2. Increased practical significance level \n",
    "--> decrease # of page views\n",
    "3. Increased confidence level (1 - alpha)\n",
    "--> increase # of page views\n",
    "4. Higher sensitivity (1 - beta)\n",
    "--> increase # of page views\n",
    "\n",
    "##### Analyze the results\n",
    "- Case 1:\n",
    "N_cont = 10,072, N_exp = 9,886, X_cont = 974, X_exp = 1242; d_min = 0.02, confidence level = 95%\n",
    "--> P_pool_hat = (974 + 1242) / (10072 + 9886) = 0.111\n",
    "\n",
    "SE_pool = square root of (0.111 * (1 - 0.111)/(1/10072 + 1/9886) = 0.00445\n",
    "--> d_hat = p_exp_hat - p_cont_hat = 1242/9886 - 974/10072 = 0.0289, estimated difference\n",
    "\n",
    "m = z * SE_pool = 1.96 * 0.00445 = 0.0087, the margin of error\n",
    "and thus the CI is (d_hat - m, d_hat + m) = (0.0289 - 0.0087, 0.0289 + 0.0087), aka (0.0202, 0.0376)\n",
    "\n",
    "Given that our practical significance boundary is 0.02, and we can be confident that we have at least that big of a change at the 95% level, that is, it is both **statistically** and **practically significant**. Therefore, we would launch the new version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4f41d",
   "metadata": {},
   "source": [
    "- Case 2: neutral\n",
    "- Case 3: statistically significance, but no practical significance\n",
    "\n",
    "if the CI includes 0 --> means it's possible that the new version has no effect at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e78e2",
   "metadata": {},
   "source": [
    "##### Making decisions about uncertain data\n",
    "Inform the decision-makers; they might use stragetic business issues besides the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fdd5f",
   "metadata": {},
   "source": [
    "# II. Policy and Ethics for Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeccd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
