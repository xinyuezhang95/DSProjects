{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cfd2e3a",
   "metadata": {},
   "source": [
    "# Outline\n",
    "## 1. What is A/B testing?\n",
    "## 2. How long does it take to run an A/B test?\n",
    "## 3. Multiple testing problem\n",
    "## 4. Novelty and primacy effect\n",
    "## 5. Interference between variants\n",
    "## 6. Dealing with interference\n",
    "## 7. A step-by-step walkthrough of a real A/B testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b5f29",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "\n",
    "## 1. What is A/B testing\n",
    "### A/B tests (aka controlled experiments)\n",
    "- used in industry to make decisions\n",
    "- simplest form: control A, treatment B (control group: existing features; treatment group: new features)\n",
    "- evaluate features with a subset of users\n",
    "\n",
    "### What do data scientists do with A/B testing? \n",
    "- Typically design A/B testing with given metrics\n",
    "- involved in all A/B testing components\n",
    "    - developing a new hypothesis\n",
    "    - designing A/B tests\n",
    "    - evaluating results\n",
    "    - making decisions\n",
    "    \n",
    "## 2. Designing an A/B test\n",
    "### Hong long does it take to run an A/B teset?\n",
    "#### Step 1: Determine the sample size\n",
    "    - Type II error (beta; P(accepting H_0 when difference exists)) or power\n",
    "    - Significance level\n",
    "    - Minimum detectable effect\n",
    "    \n",
    "    - sample size = 16 * sigma^2/(delta^2)\n",
    "        (sigma: standard deviation of population; delta: difference between treatment & control)\n",
    "     \n",
    "     - How parameters influence the sample size?\n",
    "         - if sigma is larger --> more samples\n",
    "         - if delta is larger --> less samples\n",
    "     \n",
    "     - How to estimate parameters:\n",
    "         - sigma is obtained from data\n",
    "         - as for delta, we don't know that before experiments; we can use minimum detectable effect\n",
    "     \n",
    "     - Minimum detectable effect: the smallest different matters in practice, e.g., 0.1% increase in revenue\n",
    "\n",
    "#### Step 2: Use sample size and number of users --> round the duration by weeks\n",
    "\n",
    "## 3. Multiple testing problem\n",
    "### Test multiple variants of a feature:\n",
    "    - colors\n",
    "    - homepage design\n",
    "    \n",
    "### Sample scenario:\n",
    "10 tests are running with different landing pages, including colors and designs; we can see that 1 case won and the p value is < 0.05. Should we make the change?\n",
    "\n",
    "#### Answer: No \n",
    "\n",
    "Given that there are **multiple testing variants**, we shouldn't use the same significance level, and the probability of false discovery increases. \n",
    "\n",
    "### Explanation of increasing probability of false discovery\n",
    "There are 3 groups, what is the chance of at least one false positive?\n",
    "\n",
    "#### Answer:\n",
    "Pr(no false positive) = (1 - 0.05)^3 = 0.95^3 = 0.857\n",
    "\n",
    "Pr(at least 1 false positive) = 1 - Pr(no false positive) = 0.143\n",
    "\n",
    "Therefore, the Type I error is over 14%\n",
    "\n",
    "### Dealing with 'multiple testing' problem:\n",
    "#### Method I: Bonferroni correction\n",
    "- significance level = original significance level/ number of tests\n",
    "- in this case: significance level = 0.05/10 = 0/005\n",
    "--> This method is too conservative. \n",
    "\n",
    "#### Method II: Control False Discovery Rate (FDR) -- mostly used with many metrics\n",
    "FDR = E(false positives/ rejections)\n",
    "\n",
    "Eg. With 200 metrics with FDR at 0.05\n",
    "    \n",
    "    - 5% False positive\n",
    "    - at least 1 false positive in 200 metrics\n",
    "    \n",
    "## 4. Novelty and Primacy effect\n",
    "### Definition:\n",
    "primacy effect (change aversion): people are reluctant to change\n",
    "\n",
    "novelty effect: people welcome the changes and use more\n",
    "\n",
    "**Note: Effects will not last long**\n",
    "\n",
    "An A/B test has larger or smaller initial effect due to novelty or primacy effect\n",
    "\n",
    "### Sample question:\n",
    "After we ran an A/B test on a new feature, the test won and we launched the change. However, after a week, the treatment effect quickly declined. What's wrong?\n",
    "\n",
    "#### Answer: Novelty effect\n",
    "The repeat usage declined when effect wears off. \n",
    "\n",
    "### Ways to rule out the possibility:\n",
    "- Run tests only on fist time users\n",
    "- If the test is already running: compare fist time users to old users in treatment group\n",
    "\n",
    "## 5. Interference between variants\n",
    "Typical A/B testing design: split users randomly, since users are independent (assumption)\n",
    "\n",
    "### Cases when the assumption fails:\n",
    "- social network, eg., Facebook, LinkedIn, Twitter\n",
    "- two-sided markets, eg., uber, lyft, airbnb\n",
    "\n",
    "### Sample question:\n",
    "We want to test a new feature to increase posts created per user. We assign each user randomly. The test won by 1% in terms of posts. \n",
    "\n",
    "What would happen after new feature is launched to all users? Will it be the same as 1%? Assume no novelty effect\n",
    "\n",
    "#### Answer: The difference will not be 1%\n",
    "\n",
    "Network effect:\n",
    "- User behaviors are impacted by others\n",
    "- The effect can spill over the control group\n",
    "- The difference underestimates the treatment effect\n",
    "Therefore, the difference will be more than 1%\n",
    "\n",
    "Two-sided markets\n",
    "- Resources are shared among control and treatment groups, eg., if treatment group attracts more drivers, less drivers will be available for control group\n",
    "Therefore, Actual effect < treatment effects\n",
    "\n",
    "## 6. Dealing with Interference\n",
    "### Sample question:\n",
    "A new feature is to provide coupons to our riders. Our goal is to increase rides by decreasing price. \n",
    "\n",
    "Testing strategy: evaluate the effect of the new feature\n",
    "\n",
    "#### Main idea:\n",
    "- Isolate users\n",
    "\n",
    "##### Two-sided markets:\n",
    "*Method I: Geo-based randomization:*\n",
    "- Split by geolocations\n",
    "- Eg. New York vs San Francisco\n",
    "- Big variance since markets are unique\n",
    "\n",
    "*Method II: Time-based randomization:*\n",
    "- Split by day of wekk\n",
    "- Assign all users to either treatment or control\n",
    "- Only when treatment effect is in short time\n",
    "- Does not work when treatment effect takes a long time, eg., through a referral program\n",
    "\n",
    "##### Social network:\n",
    "*Method I: Create network clusters:*\n",
    "    \n",
    "    - people interact mostly within the cluster\n",
    "    - assign clusters randomly\n",
    "\n",
    "*Method II: Ego-network randomization:*\n",
    "    \n",
    "    - originated from Linkedin\n",
    "    - A cluster is composed of an \"ego\" and her \"alters\"\n",
    "    - One-out network effect: user either has the feature or not\n",
    "    - It's simpler and more scalable\n",
    "\n",
    "### To sum up, \n",
    "- all methods have limitations\n",
    "- we need to evaluate methods based on scenarios\n",
    "- it's possible to combine methods to get a more reliable result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53de15",
   "metadata": {},
   "source": [
    "## 7. A step-by-step walkthrough of a real A/B testing process\n",
    "### Steps:\n",
    "#### 1. Prerequisiities \n",
    "#### 2. Experiment Design\n",
    "#### 3. Running Experiment\n",
    "#### 4. Result to Decision\n",
    "#### 5. Post-launch Monitoring\n",
    "______________________________________________________________________\n",
    "\n",
    "### 1. Prerequisiities \n",
    "- Understand Objective & Key Metrics\n",
    "- Variants\n",
    "- Randomization units\n",
    "\n",
    "#### Key Matric\n",
    "- Normalize revenue by # of users\n",
    "- Revenue per user\n",
    "\n",
    "#### Product Variants\n",
    "Control group & Treatment grouop\n",
    "\n",
    "#### Randomization units\n",
    "- Users (Assume enough users)\n",
    "\n",
    "### 2. Experiment Design\n",
    "#### 1). Users to target:\n",
    "- all users vs specific segment of users\n",
    "\n",
    "#### 2). Which population to select from?\n",
    "- Eg., if we want to change the website checkout design, we should select users on the 'Checkout' page, instead of the ones still browsing products\n",
    "\n",
    "#### 3). Practical significance boundary\n",
    "- Assume practical significance: Revenue increase by &#36;2 per user\n",
    "- Power of the test: 80%, Significan level: 5% \n",
    "\n",
    "--> sample size = 16 * sigma^2/(delta^2)\n",
    "\n",
    "(sigma: standard deviation of population; delta: difference between treatment & control)\n",
    "\n",
    "#### 4). Duration to run the experiment\n",
    "##### Ramp-up plan: \n",
    "No bugs & traffic can be handled\n",
    "- Expose to a small population at first (Start with dozens of users)\n",
    "- Gradually increase percentage\n",
    "- Day of week effect (eg., people make more purchases during the weekend) --> Recommended: run experiment for >= 1 whole week\n",
    "- Seasonality (holiday season) --> Data during holidays: cannot be used for analysis; run experiment longer\n",
    "- Primacy and novelty effects (Users respond to changes differently)\n",
    "\n",
    "##### Conclusion: \n",
    "- Run the experiment for >= 1 week\n",
    "- starting with 5% in both groups, and will eventually be about 33% in all groups (eg., control, treatment 1, treatment 2)\n",
    "- the experiment can be longer when there is novelty or primacy effect\n",
    "- Number of unique users: OK to get more users in each group; Recommended to be overpowered\n",
    "- Running the experiment too long --> precision won't improve further results\n",
    "\n",
    "### 3. Running the experiment\n",
    "\n",
    "### 4. Results to decision\n",
    "#### firstly, sanity check\n",
    "Why sanity checks?\n",
    "- unreliable if assumptions are violated\n",
    "\n",
    "Things need to check:\n",
    "- number of users assigned to groups\n",
    "- latency when loading the webpage\n",
    "\n",
    "#### secondly, use hypothesis testing to make recommendations\n",
    "- Recommend launching a change when the results are both statistically and practically significant\n",
    "- if uncentainty --> recommendation: don't launch the change; run a follow-up test with more power\n",
    "\n",
    "**statistically significant**: p-value\n",
    "\n",
    "**practically significant**: point estimate vs practical significance boundary\n",
    "\n",
    "### 5. Post-launch Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1c72c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c056d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
